# PLUM: Improving Inference Efficiency By Leveraging Repetition-Sparsity Trade-Off

### [Project Page](https://github.com/sachitkuhar/PLUM) | [Paper](https://openreview.net/pdf?id=IEKtMMSblm) | [Video](https://www.youtube.com/watch?v=nE_CYDWqQ_I) 
PyTorch implementation of PLUM, a quantization-system co-design framework aimed to improve inference efficiency of deep neural networks.

[PLUM: Improving Inference Efficiency By Leveraging Repetition-Sparsity Trade-Off](https://github.com/sachitkuhar/PLUM)  
 [Sachit Kuhar](https://sachitkuhar.github.io/)<sup></sup>,
 [Yash Jain](https://jinga-lala.github.io/),
 [Alexey Tumanov](https://github.com/sachitkuhar/PLUM)<br>
 Georgia Institute of Technology <br>

## BibTeX

```
@article{
kuhar2024plum,
title={{PLUM}: Improving Inference Efficiency By Leveraging Repetition-Sparsity Trade-Off},
author={Sachit Kuhar and Yash Jain and Alexey Tumanov},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=IEKtMMSblm},
note={}
}
```
